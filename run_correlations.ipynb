{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulate_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self, dataset_name='imagenet_val', target_name='resnet50_imagenet', target_layer='layer4',\n",
    "                 clip_name='ViT-SO400M-14-SigLIP-384', device='cuda', batch_size=64,\n",
    "                 activations_dir='saved_activations', explanation_method='le_label',\n",
    "                 explanation_path=None):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.target_name = target_name\n",
    "        self.target_layer = target_layer\n",
    "        self.clip_name = clip_name\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.activations_dir = activations_dir\n",
    "        self.explanation_method = explanation_method\n",
    "        self.explanation_path = explanation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:02<00:00, 93.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanations from: results/baselines/NetDissect/resnet50_imagenet_layer1.csv\n",
      "Average correlation: 0.0349\n",
      "Using existing split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:02<00:00, 104.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanations from: results/baselines/MILAN/resnet50_imagenet_layer1.csv\n",
      "Average correlation: 0.0430\n",
      "Using existing split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:02<00:00, 103.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanations from: results/baselines/CLIP_Dissect/resnet50_imagenet_layer1.csv\n",
      "Average correlation: 0.0674\n",
      "Using existing split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:03<00:00, 79.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanations from: results/ours/le_label_resnet50_imagenet_layer1.csv\n",
      "Average correlation: 0.1008\n",
      "Using existing split\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m target_model\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_val\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m args \u001b[38;5;241m=\u001b[39m Arguments(dataset_name\u001b[38;5;241m=\u001b[39mdataset_name, target_name\u001b[38;5;241m=\u001b[39mtarget_model, target_layer\u001b[38;5;241m=\u001b[39mtarget_layer, explanation_method\u001b[38;5;241m=\u001b[39mmethod)\n\u001b[1;32m----> 6\u001b[0m \u001b[43msimulate_correlation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_correlations\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\code\\linear_explanation\\simulate_correlation.py:57\u001b[0m, in \u001b[0;36msimulate_correlations\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     55\u001b[0m target_activations \u001b[38;5;241m=\u001b[39m target_activations[test_ids]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#get CLIP image features\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m clip_model, _, clip_preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mopen_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCN_TO_CHECKPOINT\u001b[49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m clip_data \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mget_data(args\u001b[38;5;241m.\u001b[39mdataset_name, clip_preprocess)\n\u001b[0;32m     61\u001b[0m clip_save_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mactivations_dir, args\u001b[38;5;241m.\u001b[39mdataset_name, args\u001b[38;5;241m.\u001b[39mclip_name)\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\open_clip\\factory.py:382\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, **model_kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_and_transforms\u001b[39m(\n\u001b[0;32m    359\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    360\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    378\u001b[0m ):\n\u001b[0;32m    379\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[0;32m    380\u001b[0m         {}, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, interpolation\u001b[38;5;241m=\u001b[39mimage_interpolation, resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode)\n\u001b[1;32m--> 382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     pp_cfg \u001b[38;5;241m=\u001b[39m PreprocessCfg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mpreprocess_cfg)\n\u001b[0;32m    402\u001b[0m     preprocess_train \u001b[38;5;241m=\u001b[39m image_transform_v2(\n\u001b[0;32m    403\u001b[0m         pp_cfg,\n\u001b[0;32m    404\u001b[0m         is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    405\u001b[0m         aug_cfg\u001b[38;5;241m=\u001b[39maug_cfg,\n\u001b[0;32m    406\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\open_clip\\factory.py:248\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m         model \u001b[38;5;241m=\u001b[39m CoCa(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_cfg, cast_dtype\u001b[38;5;241m=\u001b[39mcast_dtype)\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTextCLIP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m     model \u001b[38;5;241m=\u001b[39m CLIP(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_cfg, cast_dtype\u001b[38;5;241m=\u001b[39mcast_dtype)\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\open_clip\\model.py:327\u001b[0m, in \u001b[0;36mCustomTextCLIP.__init__\u001b[1;34m(self, embed_dim, vision_cfg, text_cfg, quick_gelu, init_logit_scale, init_logit_bias, cast_dtype, output_dict)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dict \u001b[38;5;241m=\u001b[39m output_dict\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual \u001b[38;5;241m=\u001b[39m \u001b[43m_build_vision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquick_gelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mcontext_length\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\open_clip\\model.py:119\u001b[0m, in \u001b[0;36m_build_vision_tower\u001b[1;34m(embed_dim, vision_cfg, quick_gelu, cast_dtype)\u001b[0m\n\u001b[0;32m    116\u001b[0m act_layer \u001b[38;5;241m=\u001b[39m QuickGELU \u001b[38;5;28;01mif\u001b[39;00m quick_gelu \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mGELU\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vision_cfg\u001b[38;5;241m.\u001b[39mtimm_model_name:\n\u001b[1;32m--> 119\u001b[0m     visual \u001b[38;5;241m=\u001b[39m \u001b[43mTimmModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_model_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_proj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproj_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_drop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vision_cfg\u001b[38;5;241m.\u001b[39mlayers, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m    132\u001b[0m     vision_heads \u001b[38;5;241m=\u001b[39m vision_cfg\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m vision_cfg\u001b[38;5;241m.\u001b[39mhead_width\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\open_clip\\timm_model.py:65\u001b[0m, in \u001b[0;36mTimmModel.__init__\u001b[1;34m(self, model_name, embed_dim, image_size, pool, proj, proj_bias, drop, drop_path, patch_drop, pretrained)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m extra_proj \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m custom_pool:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# use network classifier head as projection if no proj specified and no custom pooling used\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# if projection is explicitly set to \"none\" will be pass through from network trunk\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     proj_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m proj \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m embed_dim\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproj_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtimm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     prev_chs \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\_factory.py:114\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[1;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[0;32m    122\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\vision_transformer.py:2561\u001b[0m, in \u001b[0;36mvit_so400m_patch14_siglip_384\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m   2556\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvit_so400m_patch14_siglip_384\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VisionTransformer:\n\u001b[0;32m   2558\u001b[0m     model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   2559\u001b[0m         patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1152\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m27\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, mlp_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.7362\u001b[39m, class_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, global_pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2560\u001b[0m     )\n\u001b[1;32m-> 2561\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2562\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_so400m_patch14_siglip_384\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\vision_transformer.py:1727\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[1;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiglip\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m variant \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_pool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1725\u001b[0m     strict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model_with_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mVisionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\_builder.py:385\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[1;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 385\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_cls(cfg\u001b[38;5;241m=\u001b[39mmodel_cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\vision_transformer.py:531\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[1;34m(self, img_size, patch_size, in_chans, num_classes, global_pool, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_norm, init_values, class_token, no_embed_class, reg_tokens, pre_norm, fc_norm, dynamic_img_size, dynamic_img_pad, drop_rate, pos_drop_rate, patch_drop_rate, proj_drop_rate, attn_drop_rate, drop_path_rate, weight_init, embed_layer, norm_layer, act_layer, block_fn, mlp_layer)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, num_classes) \u001b[38;5;28;01mif\u001b[39;00m num_classes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_init \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_init\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\vision_transformer.py:539\u001b[0m, in \u001b[0;36mVisionTransformer.init_weights\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    538\u001b[0m     nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m--> 539\u001b[0m \u001b[43mnamed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_init_weights_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_bias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\_manipulate.py:34\u001b[0m, in \u001b[0;36mnamed_apply\u001b[1;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child_module \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m     33\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((name, child_name)) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mnamed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth_first \u001b[38;5;129;01mand\u001b[39;00m include_root:\n\u001b[0;32m     36\u001b[0m     fn(module\u001b[38;5;241m=\u001b[39mmodule, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\_manipulate.py:34\u001b[0m, in \u001b[0;36mnamed_apply\u001b[1;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child_module \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m     33\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((name, child_name)) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mnamed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth_first \u001b[38;5;129;01mand\u001b[39;00m include_root:\n\u001b[0;32m     36\u001b[0m     fn(module\u001b[38;5;241m=\u001b[39mmodule, name\u001b[38;5;241m=\u001b[39mname)\n",
      "    \u001b[1;31m[... skipping similar frames: named_apply at line 34 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\_manipulate.py:34\u001b[0m, in \u001b[0;36mnamed_apply\u001b[1;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child_module \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m     33\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((name, child_name)) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mnamed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth_first \u001b[38;5;129;01mand\u001b[39;00m include_root:\n\u001b[0;32m     36\u001b[0m     fn(module\u001b[38;5;241m=\u001b[39mmodule, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\_manipulate.py:36\u001b[0m, in \u001b[0;36mnamed_apply\u001b[1;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[0;32m     34\u001b[0m     named_apply(fn\u001b[38;5;241m=\u001b[39mfn, module\u001b[38;5;241m=\u001b[39mchild_module, name\u001b[38;5;241m=\u001b[39mchild_name, depth_first\u001b[38;5;241m=\u001b[39mdepth_first, include_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth_first \u001b[38;5;129;01mand\u001b[39;00m include_root:\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\models\\vision_transformer.py:693\u001b[0m, in \u001b[0;36minit_weights_vit_timm\u001b[1;34m(module, name)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m--> 693\u001b[0m     \u001b[43mtrunc_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mzeros_(module\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\layers\\weight_init.py:67\u001b[0m, in \u001b[0;36mtrunc_normal_\u001b[1;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fills the input Tensor with values drawn from a truncated\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03mnormal distribution. The values are effectively drawn from the\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mnormal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    >>> nn.init.trunc_normal_(w)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_trunc_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tuoma\\miniconda3\\envs\\pytorch2\\Lib\\site-packages\\timm\\layers\\weight_init.py:28\u001b[0m, in \u001b[0;36m_trunc_normal_\u001b[1;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[0;32m     24\u001b[0m u \u001b[38;5;241m=\u001b[39m norm_cdf((b \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Uniformly fill tensor with values from [l, u], then translate to\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# [2l-1, 2u-1].\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Use inverse cdf transform for normal distribution to get truncated\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# standard normal\u001b[39;00m\n\u001b[0;32m     32\u001b[0m tensor\u001b[38;5;241m.\u001b[39merfinv_()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for target_model in [\"resnet50_imagenet\", \"resnet18_places365\"]:\n",
    "    for target_layer in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        for method in [\"net_dissect\", \"milan\", \"clip_dissect\", \"le_label\", \"le_siglip\"]:\n",
    "            dataset_name = target_model.split(\"_\")[-1] + \"_val\"\n",
    "            args = Arguments(dataset_name=dataset_name, target_name=target_model, target_layer=target_layer, explanation_method=method)\n",
    "            simulate_correlation.simulate_correlations(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
